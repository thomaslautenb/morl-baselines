{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\anaconda3\\envs\\MORL_baseline\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MOHalfCheehtahEnv' object has no attribute 'reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#reward_dim = envs.single_reward_space.shape[0]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Create a single environment instance to get reward space information\u001b[39;00m\n\u001b[0;32m     37\u001b[0m single_env \u001b[38;5;241m=\u001b[39m mo_gym\u001b[38;5;241m.\u001b[39mmake(env_id)\n\u001b[1;32m---> 38\u001b[0m reward_dim \u001b[38;5;241m=\u001b[39m \u001b[43msingle_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m network \u001b[38;5;241m=\u001b[39m MOPPONet(obs_shape, action_shape, reward_dim)\n\u001b[0;32m     41\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;241m*\u001b[39m reward_dim)  \u001b[38;5;66;03m# Example weights, can be adjusted\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MORL_baseline\\Lib\\site-packages\\gymnasium\\core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` that will search the reminding wrappers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m )\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MORL_baseline\\Lib\\site-packages\\gymnasium\\core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` that will search the reminding wrappers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m )\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MOHalfCheehtahEnv' object has no attribute 'reward'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mo_gymnasium as mo_gym\n",
    "import torch as th\n",
    "import time\n",
    "from mo_gymnasium import MORecordEpisodeStatistics\n",
    "from torch import nn\n",
    "from morl_baselines.common.networks import mlp\n",
    "\n",
    "\n",
    "from morl_baselines.single_policy.ser import mo_ppo\n",
    "# Define the make_env function\n",
    "def make_env(env_id, seed, idx, run_name, gamma):\n",
    "    def thunk():\n",
    "        env = mo_gym.make(env_id)\n",
    "        env = MORecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed + idx)\n",
    "        env.observation_space.seed(seed + idx)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# Step 1: Create the MO Gym environment\n",
    "env_id = \"mo-halfcheetah-v4\"\n",
    "num_envs = 8\n",
    "seed = 123\n",
    "gamma = 0.99\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, seed, idx, f\"run_{seed}\", gamma) for idx in range(num_envs)]\n",
    ")\n",
    "\n",
    "# Step 2: Initialize the agent\n",
    "obs_shape = envs.single_observation_space.shape\n",
    "action_shape = envs.single_action_space.shape\n",
    "#reward_dim = envs.single_reward_space.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single environment instance to get reward space information\n",
    "single_env = mo_gym.make(env_id)\n",
    "single_env.reset()\n",
    "obs, reward, terminated, truntuated, info = single_env.step(action=(1,1,1,1,1,1))\n",
    "reward_dim = reward.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n\u001b[1;32m---> 34\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 595\u001b[0m, in \u001b[0;36mMOPPO.train\u001b[1;34m(self, start_time, current_iteration, max_iterations)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lrnow\n\u001b[0;32m    594\u001b[0m \u001b[38;5;66;03m# Fills buffer\u001b[39;00m\n\u001b[1;32m--> 595\u001b[0m next_obs, next_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__collect_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Compute advantage on collected samples\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__compute_advantages(next_obs, next_done)\n",
      "Cell \u001b[1;32mIn[1], line 407\u001b[0m, in \u001b[0;36mMOPPO.__collect_samples\u001b[1;34m(self, obs, done)\u001b[0m\n\u001b[0;32m    404\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetworks\u001b[38;5;241m.\u001b[39mreward_dim)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# Perform action on the environment\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m next_obs, reward, next_terminated, _, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m reward \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetworks\u001b[38;5;241m.\u001b[39mreward_dim)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# storing to batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MORL_baseline\\Lib\\site-packages\\gymnasium\\vector\\vector_env.py:204\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    {}\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MORL_baseline\\Lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:145\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], {}\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[0;32m    143\u001b[0m     (\n\u001b[0;32m    144\u001b[0m         observation,\n\u001b[1;32m--> 145\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i],\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i],\n\u001b[0;32m    148\u001b[0m         info,\n\u001b[0;32m    149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i]:\n\u001b[0;32m    152\u001b[0m         old_observation, old_info \u001b[38;5;241m=\u001b[39m observation, info\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "network = MOPPONet(obs_shape, action_shape, reward_dim)\n",
    "weights = np.array([1.0] * reward_dim)  # Example weights, can be adjusted\n",
    "\n",
    "agent = MOPPO(\n",
    "    id=1,\n",
    "    networks=network,\n",
    "    weights=weights,\n",
    "    envs=envs,\n",
    "    log=False,\n",
    "    steps_per_iteration=2048,\n",
    "    num_minibatches=32,\n",
    "    update_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    anneal_lr=False,\n",
    "    clip_coef=0.2,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    clip_vloss=True,\n",
    "    max_grad_norm=0.5,\n",
    "    norm_adv=True,\n",
    "    target_kl=None,\n",
    "    gae=True,\n",
    "    gae_lambda=0.95,\n",
    "    device=\"cpu\",  # Adjust based on your hardware\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Step 3: Train the agent\n",
    "max_iterations = 1000\n",
    "start_time = time.time()\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    agent.train(start_time, current_iteration=iteration, max_iterations=max_iterations)\n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Iteration {iteration} completed\")\n",
    "\n",
    "# Step 4: Evaluate the agent\n",
    "obs = envs.reset(seed=seed)\n",
    "done = False\n",
    "total_reward = np.zeros(reward_dim)\n",
    "\n",
    "while not done:\n",
    "    action = agent.eval(obs, weights)\n",
    "    obs, reward, done, _, info = envs.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MORL_baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
